{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNON8J1VPaXRa2E8s4rmZ1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsa-nut/SF323_CN408_AIEngineer/blob/main/SF323_CN408_HW4_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW4 - RAG (5 points)\n",
        "\n",
        "---\n",
        "\n",
        "ในการบ้านนี้เราจะมาฝึกฝนการทำ RAG โดยการบ้านจะแบ่งเป็นขั้นตอนต่อไปนี้\n",
        "\n",
        "1. Generate synthetic questions: สร้างคำถามจากข้อมูลที่มีเพื่อไว้ทดสอบระบบ\n",
        "\n",
        "2. Building RAG\n",
        "\n",
        "3. Evaluate RAG\n",
        "\n",
        "    3.1  Evaluate the retriever\n",
        "\n",
        "    3.2  Evaluate the answer\n",
        "\n",
        "การบ้านนี้ยาวแนะนำให้เริ่มต้นแต่เนิ่นๆ"
      ],
      "metadata": {
        "id": "Ill52XAzGbw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {
        "id": "niLASI4lOa0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pinecone"
      ],
      "metadata": {
        "id": "m7OuhyqeIxBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import random\n",
        "import re\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=userdata.get('openrouter'),\n",
        ")\n",
        "\n",
        "def generate(prompt: str):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"google/gemini-2.5-flash-lite\",\n",
        "        messages = [{'role':'user',\n",
        "                     'content':prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "generate(\"Hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p4V-Hq0YOfda",
        "outputId": "81d3bc94-43f9-4437-920a-5aebf5c8dc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generate synthetic questions (0.5 points)\n",
        "\n",
        "ใน part แรก เราจะมาสร้าง Question Answer pairs เพื่อใช้สำหรับตรวจสอบ RAG. โดยทรัพยากรที่จำกัด เราจะสร้างแค่ 30 คู่เท่านั้น โดยในการสร้าง คำถามเราจะใช้ LLM (gemini-2.5-flash-lite) สร้าง และ เราจะใช้ LLM มาตรวจสอบคำถามว่าเหมาะสมไหม"
      ],
      "metadata": {
        "id": "xiwhMfLQHQEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 โหลดข้อมูลที่จะใช้ทำ RAG. ข้อมูลเป็นหนังสือ Biochemistry จาก [MedQA](https://github.com/jind11/MedQA)"
      ],
      "metadata": {
        "id": "oelaQepbWLCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkG2j1dwGHPA"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/yongsa-nut/SF323_CN408_AIEngineer/refs/heads/main/Biochemistry_Lippincott.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 โหลด data แล้วตัดให้เป็น chunk ด้วย recursivesplitting"
      ],
      "metadata": {
        "id": "Us8BpLf8WpLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "with open('Biochemistry_Lippincott.txt', 'r') as file:\n",
        "    biochem_textbook = file.read()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Big chunk for generating questions\n",
        "    chunk_overlap=100,\n",
        "    add_start_index=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "doc_chunks = text_splitter.split_text(biochem_textbook)"
      ],
      "metadata": {
        "id": "xCpFG4clWpdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 สร้าง prompt สำหรับ QA"
      ],
      "metadata": {
        "id": "agvtv4kLYVcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_QA_generation_prompt(context):\n",
        "\n",
        "    QA_generation_prompt = f\"\"\"You are a biology professor creating exam questions from a biochemistry textbook.\n",
        "\n",
        "Your task is to write a factual exam question and answer based on the provided context.\n",
        "\n",
        "Context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Provide your response in JSON format:\n",
        "{{\n",
        "  \"question\": \"your_question\",\n",
        "  \"answer\": \"your_answer\",\n",
        "  \"reference\": \"the part of the context where the answer come from\"\n",
        "}}\n",
        "\n",
        "**Important Requirements**:\n",
        "- The question must be answerable with specific, concise factual information from the context\n",
        "- The question must be self-contained (no references to \"the passage\" or \"the context\" such as \"According to the context\" or \"According to the provided information\")\n",
        "- The answer must come directly from the provided context.\n",
        "- The reference should include only the key part without anything extra like headers or numbers.\n",
        "- The reference MUST match exactly the part of the context without any extra strings or ...\n",
        "- Keep the answer concise\n",
        "\n",
        "\n",
        "If the context is unsuitable for creating exam questions without knowing the context, return: \"BAD CONTEXT\"\n",
        "\"\"\"\n",
        "    return QA_generation_prompt"
      ],
      "metadata": {
        "id": "ooJ9TTMgXi8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "context = random.sample(doc_chunks, 1)[0] # ลองทดสอบ ด้วย context ที่ แรนด้อม เช่น sdfsdfad ดูว่าโมเดลตอบ \"BAD CONTEXT\" ไหม\n",
        "qa_prompt = gen_QA_generation_prompt(context)\n",
        "response = generate(qa_prompt)\n",
        "\n",
        "print(f'Context: {context}\\n')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "UaaL1NnMiAjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 สร้าง prompt สำหรับเช็ค คำถาม"
      ],
      "metadata": {
        "id": "tbbjSyhmaAe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_eval_question_prompt(context, question):\n",
        "    eval_question_prompt = f\"\"\"Given the following context and exam question, evaluate whether the question meets these criteria:\n",
        "\n",
        "1. **Groundedness**: Can the question be answered using only the provided context?\n",
        "2. **Relevance**: Is the question a suitable exam question that tests important concepts and in the question format?\n",
        "3. **Stand-alone**: Can someone with domain knowledge understand and answer this question **without seeing this specific context**? The question must not include something like \"According to the context...\" or \"According to the provided information ...\".\n",
        "\n",
        "Context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Think carefully about each criteria. Then, respond with Yes or No for each criterion in JSON format:\n",
        "{{\n",
        "  \"groundedness\": \"Yes/No\",\n",
        "  \"relevance\": \"Yes/No\",\n",
        "  \"stand_alone\": \"Yes/No\"\n",
        "}}\n",
        "\"\"\"\n",
        "    return eval_question_prompt"
      ],
      "metadata": {
        "id": "yalYi84DaD-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to extract json from the output\n",
        "def extract_json_from_output(output_string):\n",
        "    try:\n",
        "        return json.loads(output_string)\n",
        "    except:\n",
        "        # Find content between ```json and ```\n",
        "        pattern = r'```json\\s*(.*?)\\s*```'\n",
        "        match = re.search(pattern, output_string, re.DOTALL)\n",
        "\n",
        "        if match:\n",
        "            json_str = match.group(1)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing JSON: {e}\")\n",
        "                return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "7EQmLpiMpjQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "clean_response = json.loads(response.replace(\"```json\",\"\").replace(\"```\",\"\"))\n",
        "eval_prompt = gen_eval_question_prompt(context, clean_response['question'])\n",
        "result = generate(eval_prompt)\n",
        "\n",
        "print(eval_prompt)\n",
        "print('-----')\n",
        "print(result)\n",
        "print('-----')\n",
        "print(extract_json_from_output(result))"
      ],
      "metadata": {
        "id": "rreoy402kzrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5 สร้างคำถามจนกว่า จะได้คำถามที่ผ่านเกณฑ์ทั้งหมด 30 คำถาม\n",
        "\n",
        "### **Code ด้านล่างสร้าง eval dataset ซึ่งไม่ต้องรัน เพราะ Data สร้างไว้ให้แล้ว**\n",
        "\n",
        "สามารถกดดูได้ถ้าสนใจ"
      ],
      "metadata": {
        "id": "gBA0gGz_aEbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_QUESTIONS = 30\n",
        "\n",
        "random.seed(77)\n",
        "questions = []\n",
        "count = 1\n",
        "while len(questions) != NUM_QUESTIONS:\n",
        "    count += 1\n",
        "    # randomly draw a chunk\n",
        "    context = random.sample(doc_chunks, 1)[0]\n",
        "\n",
        "    # Gen a question-answer pair\n",
        "    qa_prompt = gen_QA_generation_prompt(context)\n",
        "    answer = generate(qa_prompt)\n",
        "\n",
        "    if answer == \"BAD CONTEXT\":\n",
        "        continue\n",
        "    clean_answer = json.loads(answer.replace(\"```json\",\"\").replace(\"```\",\"\"))\n",
        "\n",
        "    # Check the question-answer pair\n",
        "    eval_prompt = gen_eval_question_prompt(context, clean_answer['question'])\n",
        "    result = generate(eval_prompt)\n",
        "    json_result = extract_json_from_output(result)\n",
        "    if json_result is None:\n",
        "        continue\n",
        "\n",
        "    if (json_result['groundedness'] == 'Yes' and\n",
        "          json_result['relevance'] == 'Yes' and\n",
        "          json_result['stand_alone'] == 'Yes'):\n",
        "\n",
        "        clean_answer['context'] = context\n",
        "        questions.append(clean_answer)\n",
        "\n",
        "print(f\"Number of attempts: {count}\")\n",
        "df_questions = pd.DataFrame(questions)\n",
        "df_questions.to_csv('RAG_eval_df.csv')\n",
        "df_questions"
      ],
      "metadata": {
        "id": "0Y44iWP8abva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: อันนี้คือการสร้าง basic questions ซึ่งมันจะขึ้นอยู่กับแค่ chunk เดียว ในความจริง คำถามอาจจะต้องใช้หลาย chunks มาช่วยตอบ   "
      ],
      "metadata": {
        "id": "RWh1-XrKTXe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### คำถามสำหรับ Part 1"
      ],
      "metadata": {
        "id": "18RWIEj__szI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.1** (0.15 points): ข้อจำกัดของ synthetic data ตามแบบด้านบนนี้มีอะไรบ้าง คำถามแบบไหนที่จะไม่มีอยู่ในชุดข้อมูลด้านบน"
      ],
      "metadata": {
        "id": "vBXhojgxk4tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: Your answer here"
      ],
      "metadata": {
        "id": "-_d9yHpMk7O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.2**: (0.15 points) Prompt สำหรับสร้าง QA มี Output ออกมาประกอบไปด้วยอะไรบ้าง และแต่ส่วนมีการ prompt อย่างไร จงอธิบาย"
      ],
      "metadata": {
        "id": "q7cHz1h9vVl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: Your answer here"
      ],
      "metadata": {
        "id": "0og82HeUvhW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.3**: (0.2 points) เกณฑ์ในการเช็ค คำถามจาก context ที่ให้มีอะไรบ้างจงอธิบาย และ ให้คิดเกณฑ์เพิ่มอีกอย่างหนึ่งที่ควรจะเช็คเพิ่ม"
      ],
      "metadata": {
        "id": "vTISsORTvj_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**: Your answer here."
      ],
      "metadata": {
        "id": "F6GhUBT5vw-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building our RAG\n",
        "\n",
        "สำหรับส่วนนี้เราจะมาสร้าง RAG pipeline กัน"
      ],
      "metadata": {
        "id": "Aa-RrPMdOfxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data again in case you start looking from here\n",
        "!wget https://raw.githubusercontent.com/yongsa-nut/SF323_CN408_AIEngineer/refs/heads/main/Biochemistry_Lippincott.txt"
      ],
      "metadata": {
        "id": "BYt1ZtR765jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Biochemistry_Lippincott.txt', 'r') as file:\n",
        "    biochem_textbook = file.read()"
      ],
      "metadata": {
        "id": "VQ1q_nTkOk3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Preparing the data"
      ],
      "metadata": {
        "id": "3tT4I1S-buf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ในขั้นตอนถัดไปเราจะมาเตรียมข้อมูลให้พร้อม ก่อนที่จะเอาไป upend ไปที่ database\n",
        "- ขั้นตอนหลักคือเราจะต้องแบ่งบทความเป็นส่วนย่อๆแทนที่จะใช้ทั้งบทความไป embed\n",
        "- เราจะทดสอบสองแบบ และจะใช้ `Langchain` library มาช่วย\n",
        " - แบบแรกคือ ตัดเป็นความยาวเท่าๆกัน ใช้ `CharacterTextSplitter` ([Documentation](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html))\n",
        " - แบบสองคือ ตัดแบบrecursiveตาม structure ใช้ `RecursiveCharacterTextSplitter` ([Documentation](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html))\n",
        "- parameter ที่สำคัญคือ `chunk` หรือความยาวของประโยคที่จะตัด\n"
      ],
      "metadata": {
        "id": "ydqUoU80bwlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ลองทดสอบกับบทความแรก"
      ],
      "metadata": {
        "id": "n8Ou6l2rekqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "chunk_size = 500 # Test different numbers\n",
        "chunk_overlab = 50\n",
        "\n",
        "char_splitter = CharacterTextSplitter(chunk_size = chunk_size,\n",
        "                                      chunk_overlap=chunk_overlab,\n",
        "                                      separator='', #character that you would like to split on\n",
        "                                      strip_whitespace=True)\n",
        "\n",
        "char_chunks = char_splitter.split_text(biochem_textbook)\n",
        "\n",
        "print(len(char_chunks))\n",
        "char_chunks[:10]"
      ],
      "metadata": {
        "id": "TaKKfx-dfg_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Default separators สำหรับ `RecursiveCharacterTextSplitter` นั้นมี แค่ `[\"\\n\\n\", \"\\n\", \" \", \"\"]` เราเลยเพิ่ม `.` ลงไปด้วย"
      ],
      "metadata": {
        "id": "dcJN2f5li-DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 500 # Test different numbers\n",
        "chunk_overlap = 50\n",
        "\n",
        "recur_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size,\n",
        "                                                chunk_overlap=chunk_overlap,\n",
        "                                                strip_whitespace=True,\n",
        "                                                separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "                                                )\n",
        "recur_chunks = recur_splitter.split_text(biochem_textbook)\n",
        "\n",
        "print(len(recur_chunks))\n",
        "recur_chunks[:10]"
      ],
      "metadata": {
        "id": "lIQrVNgjgIlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- สามารถทดสอบดูได้ที่ web นี้ https://chunkviz.up.railway.app/"
      ],
      "metadata": {
        "id": "5sxVz9rFhpL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating splitted document (0.25 points)\n",
        "- ถัดไปเราจะสร้างจริง เราจะตัดบทความทั้งหมดสองแบบเพื่อไว้ทดสอบว่าแบบไหนดีกว่า   \n",
        "  - แบบแรกคือ `CharacterTextSpliter`\n",
        "  - แบบสองคือ `RecursiveTextSpliter`\n",
        "- `chunk_size` เราจะตั้งไว้ที่ 500 และ `chunk_overlab` เป็น 50\n"
      ],
      "metadata": {
        "id": "z00CYfHJiosO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 500\n",
        "chunk_overlab = 50\n",
        "\n",
        "char_splitter = ...\n",
        "\n",
        "char_chunks = ...\n",
        "\n",
        "recur_splitter = ...\n",
        "\n",
        "recur_chunks = ...\n",
        "\n",
        "split_docs = {'char': char_chunks,\n",
        "              'recur': recur_chunks}"
      ],
      "metadata": {
        "id": "ZCj-J3lmifuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_docs['char'])"
      ],
      "metadata": {
        "id": "xqI6rUSMq57R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_docs['recur'])"
      ],
      "metadata": {
        "id": "XuNWzIoXq8yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Embedding Data\n",
        "\n",
        "- **Note**: ต้องใช้ Hugging Face Token ([here](https://huggingface.co/docs/hub/en/security-tokens)) สามารถตั้งเป็น secret key ใน colab ได้"
      ],
      "metadata": {
        "id": "rxC8CU9NL5Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "1nKghSPzNbZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "เราจะทดสอบ Embeddding สองตัว ดังต่อไปนี้\n",
        "- 'all-MiniLM-L6-v2'\n",
        "- 'BAAI/bge-m3' เป็นตัวที่ดีของ SentenceTransformer แต่ว่าขนาดใหญ่กว่า (See [Documentation](https://huggingface.co/BAAI/bge-m3))\n",
        "\n",
        "เนื่องด้วย documents ที่เราใช้นั้นใหญ่ เราจะใช้ cuda ในการรัน\n",
        "- ไปที่ Runtime > Change runtime type > T4 GPU\n"
      ],
      "metadata": {
        "id": "aKcRSVOSOsrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device != 'cuda':\n",
        "    print('No cuda!! - Embedding time will be very long!!!')\n",
        "\n",
        "mini_embedding =  SentenceTransformer('all-MiniLM-L6-v2',  device=device)\n",
        "bge_embedding =  SentenceTransformer('BAAI/bge-m3',  device=device)"
      ],
      "metadata": {
        "id": "qe_y92BpPqXh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ทดสอบ Embedding"
      ],
      "metadata": {
        "id": "FUkQcQjQP8VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Today is a nice day.'\n",
        "mini_ec = mini_embedding.encode(query)\n",
        "bge_ec = bge_embedding.encode(query)\n",
        "\n",
        "print(mini_ec[:5])\n",
        "print(bge_ec[:5])"
      ],
      "metadata": {
        "id": "O49GXC5LP7-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Creating Embedded Documents (0.25 points)\n",
        "\n",
        "- ขั้นตอนถัดไป เราจะมาสร้าง Embedded Documents สำหรับแต่ละ splited docs (`char` and `recur`) และ embedding (`mini`, `bge`)\n",
        "- เพราะฉะนั้นจะมีด้วยกันทั้งหมด สี่อัน\n",
        "- **Note**: Code น่าจะรันนาน โดยเฉพาะถ้าคุณไม่ได้ใช้ GPU (มากกว่า 20 นาที)"
      ],
      "metadata": {
        "id": "gW9Y30KIrHXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs = { 'mini-char' : mini_embedding.encode(char_chunks),\n",
        "                  'mini-recur': ...,\n",
        "                  'bge-char'  : ... ,\n",
        "                  'bge-recur' : ... }"
      ],
      "metadata": {
        "id": "NnSIh3pZucUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone Database (1 points)\n",
        "- ในสวนนี้เราจะสร้าง pinecone database\n",
        "- ขั้นแรกคุณจะต้องไปสมัครและเอา api มาใส่ให้เรียบร้อย\n",
        "- pinecone webiste: https://www.pinecone.io/"
      ],
      "metadata": {
        "id": "wcOe6DlBXe-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- สร้าง 4 index สำหรับ 4 embedded_docs ที่เราสร้างไว้ และเก็บไว้ใน dict `indexes`\n",
        "\n",
        "**Note**: Free tier ใช้ได้แค่ 5 index"
      ],
      "metadata": {
        "id": "noSkjmGleD8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone = Pinecone(api_key=userdata.get('pinecone_key')) # from colab secret key\n",
        "\n",
        "# Store the index in the dict\n",
        "indexes = {}\n",
        "\n",
        "embeds = {'mini': mini_embedding,\n",
        "          'bge': bge_embedding}\n",
        "\n",
        "for doc in embedded_docs:\n",
        "    index_name = 'hw4-rag' + doc\n",
        "    # Cleaning up the index\n",
        "    if index_name in [index.name for index in pinecone.list_indexes()]:\n",
        "          pinecone.delete_index(index_name)\n",
        "\n",
        "    # Creating a serverless index\n",
        "    pinecone.create_index(\n",
        "        name = ...,  ## fill in here\n",
        "        dimension = embeds[doc.split('-')[0]].get_sentence_embedding_dimension(),\n",
        "        metric = ..., ## fill in here\n",
        "        spec = ServerlessSpec(cloud='aws', region='us-east-1'))\n",
        "\n",
        "    indexes[doc] = pinecone.Index(...) ## fill in here"
      ],
      "metadata": {
        "id": "ASTQVJrDcmlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- หลังจากสร้างเสร็จ เอาข้อมูลไปใส่บน database ตามที่สร้างไว้\n",
        "- ให้ unsertที่ละ 200 chunks ต่อครั้ง pinecone มีข้อจำกัด ในการ unsert"
      ],
      "metadata": {
        "id": "9u_Bj6q-FAsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "\n",
        "for doc in tqdm(embedded_docs):\n",
        "    for i in range(0, len(embedded_docs[doc]), batch_size):\n",
        "        # find end of batch\n",
        "        i_end = min(i+batch_size, len(embedded_docs[doc]))\n",
        "\n",
        "        # create IDs batch\n",
        "        ids = [str(x) for x in range(i, i_end)]\n",
        "        # create metadata batch\n",
        "        metadatas = [{'text': text} for text in split_docs[doc.split('-')[1]][i:i_end]]\n",
        "        # create embeddings\n",
        "        em_chunk = embedded_docs[doc][i:i_end]\n",
        "\n",
        "        # create records list for upsert\n",
        "        records = []\n",
        "        for x in range(len(ids)):\n",
        "            ## **Fill in your code below**\n",
        "\n",
        "\n",
        "\n",
        "        # upsert to Pinecone\n",
        "        indexes[doc].upsert(vectors=records)"
      ],
      "metadata": {
        "id": "nqEL83Lug-S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ตรวจสอบ pinecone database ว่าเรียบร้อยก่อนจะไปต่อ\n",
        "- code ข้างบนควรรันแค่ครั้งเดียว หลังจากนั้นไม่จำเป็นต้อง upsert อีก เรียกใช้ได้เลย"
      ],
      "metadata": {
        "id": "8_ZJyupcjD_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG functions (1 points)"
      ],
      "metadata": {
        "id": "5m7J8vRbFZ6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load the data for eval"
      ],
      "metadata": {
        "id": "uuFj8IJFL2WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yongsa-nut/SF323_CN408_AIEngineer/refs/heads/main/RAG_eval_df.csv"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bY-aWOZ72t7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df = pd.read_csv('RAG_eval_df.csv')"
      ],
      "metadata": {
        "id": "yreka5RQ22Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Code box ด้านล่าง โหลด all indexes ถ้ามีข้อมูลอยู่แล้วไม่จำเป็นต้องรัน code ด้านบนเพื่อ upsert ใหม่"
      ],
      "metadata": {
        "id": "jbHrm462Lc_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all indexes\n",
        "embedded_list = ['mini-char', 'mini-recur', 'bge-char','bge-recur' ]\n",
        "embeds = {'mini': mini_embedding,\n",
        "          'bge': bge_embedding}\n",
        "\n",
        "pinecone = Pinecone(api_key=userdata.get('pinecone_key'))\n",
        "\n",
        "INDEX_NAME = 'hw4-rag'\n",
        "indexs = {}\n",
        "for doc in embedded_list:\n",
        "    indexs[doc] = pinecone.Index(INDEX_NAME + doc)"
      ],
      "metadata": {
        "id": "bCRXrPBpEunG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- เติม code ในช่องด้านล่าง เพื่อ retrieve documents จาก vector database\n",
        "  1. เอา query ไป embed ด้วย `embed_model`\n",
        "  2. เอา query ไปดึง chunks ที่เก็บไว้ใน vector database (index)\n",
        "  3. ดึง metadata (text) ออกมา เก็บไว้ใน List\n",
        "  4. return list นั้นออกไป"
      ],
      "metadata": {
        "id": "HntETX2l-LxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_docs(query, embed_model, index, top_k):\n",
        "    ## Embedding the query\n",
        "    embed_query = ...\n",
        "\n",
        "    ## Retrieve documents\n",
        "    retrieved_docs = ...\n",
        "\n",
        "    # Get the actual text\n",
        "    texts = [r['metadata']['text'] for r in retrieved_docs['matches']]\n",
        "    return texts"
      ],
      "metadata": {
        "id": "nszbW1bg6jJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- เติม code ในช่องด้านล่างให้สร้าง function ที่รับ คำถาม, embedding model, database index, top_k\n",
        "  1. ไปหาว่า documents ที่ใกล้คำถามที่สุดคืออะไร\n",
        "  2. เอา documents ที่ได้มาสร้าง prompt เพื่อตอบคำถาม\n",
        "  3. เอา prompt ไป gen response แล้วก็ return response ออกมา"
      ],
      "metadata": {
        "id": "RJBRUUme-EN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG_response(query, embed_model, index, top_k=3):\n",
        "    # return the response from the model with augmented prompt\n",
        "    pass"
      ],
      "metadata": {
        "id": "J-ax8oNy6fyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "query = eval_df['question'][0]\n",
        "response = RAG_response(query, embeds['bge'], indexs['bge-char'])\n",
        "\n",
        "print('Response: ', response)\n",
        "print('\\nGround truth: ',eval_df['answer'][0])"
      ],
      "metadata": {
        "id": "_eZmSEh7MDMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluate our RAG (1 point)\n",
        "\n",
        "เพื่อให้ eval เหมือนกัน เราจะใช้ data ที่มีมาให้แล้ว"
      ],
      "metadata": {
        "id": "RBls6o4NOlDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Evaluate the Retrieval\n",
        "\n",
        "- สิ่งที่เราต้องการทดสอบมีด้วยกันทั้งหมด 3 อย่าง แต่ละอย่างมี สองค่า\n",
        "  - embedding model: `mini` หรือ `bge`\n",
        "  - spliting method: `char` หรือ `recur`\n",
        "  - top-k: `1` หรือ `5`\n",
        "- สิ่งที่เราจะคำนวณ คือ recall = relevant retrieve / total relevant. คำถามนั้นเราสร้างเอง และเรารู้ว่า มีแค่ chunk เดียวจากทั้งหมด ที่ relevant ดังน้้น ในแต่ละคำถาม ผลที่ได้จะเป็น 0 หรือ 1 เท่านั้น พูดอีกอย่างคือที่ดึงมามี context (เช็คจาก reference) หรือไม่\n",
        "\n",
        "- ดังนั้นจะมีทั้งหมด 8 ค่า เราจะสร้างรูปมาวนตรวจสอบและเก็บค่าของทั้งหมดไว้ใน `DataFrame` ดังนั้น `DataFrame` นี้จะมี 4 columns: `embedding, spliting, top-k, score` และมี 8 rows\n",
        "\n",
        "- **Note**: reference มาจากโมเดล ซึ่งโมเดลอาจจะไม่ตัดมาเป๊ะจาก context. ในการเช็ค reference ใช้ exact match up to a threshold เพื่อความง่ายและเร็ว ถ้า reference มีความซับซ้อน ควรจะใช้ตำแหน่งของข้อความ (line) หรือ ใช้โมเดลมาตรวจสอบ"
      ],
      "metadata": {
        "id": "Rnit8zl_OoEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combinations = [(em, split, k) for em in ['mini','bge'] for split in ['char','recur'] for k in [1, 5]]\n",
        "retrieval_results = []\n",
        "threshold = 20\n",
        "\n",
        "for (em, split, k) in tqdm(combinations):\n",
        "    total = 0\n",
        "    for index, row in eval_df.iterrows():\n",
        "        question = row['question']\n",
        "        retrieved_docs = retrieve_docs(question,\n",
        "                                       embeds[em],\n",
        "                                       indexs[em+'-'+split],\n",
        "                                       k)\n",
        "\n",
        "        context = \"\\n\".join(retrieved_docs)\n",
        "        # Check if the reference is in the retreive docs\n",
        "        if row['reference'][:threshold] in context:\n",
        "            total += 1\n",
        "\n",
        "    retrieval_results.append({'embedding':em,\n",
        "                    'splitting':split,\n",
        "                    'top-k':k,\n",
        "                    'avg_score':total/len(eval_df)})\n",
        "\n",
        "retrieval_results_df = pd.DataFrame(retrieval_results)\n",
        "\n",
        "retrieval_results_df"
      ],
      "metadata": {
        "id": "A_c49kDyOquy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Evaluate answers (1 points)\n",
        "\n",
        "- สิ่งที่เราต้องการทดสอบมีด้วยกันทั้งหมด 3 อย่าง แต่ละอย่างมี สองค่า\n",
        "  - embedding model: `mini` หรือ `bge`\n",
        "  - spliting method: `char` หรือ `recur`\n",
        "  - top-k: `1` หรือ `5`\n",
        "- ดังนั้นจะมีทั้งหมด 8 ค่า เราจะสร้างรูปมาวนตรวจสอบและเก็บค่าของทั้งหมดไว้ใน `DataFrame` ดังนั้น `DataFrame` นี้จะมี 4 columns: `embedding, spliting, top-k, score` และมี 8 rows\n",
        "- เราใช้ training มาตรวจสอบ\n",
        "- ถัดไปในการ eval นี้ เราจะใช้ LLM มาตรวจว่าคำตอบถูกต้องสมบูรณ์ไหน\n",
        "- **Task**: สิ่งสำคัญคือ eval prompt สำหรับ LLM as a judge โดยที่ prompt ที่จะให้สร้างมีข้อกำหนดดังนี้\n",
        "  - เราจะตรวจสองแค่สิ่งเดียวคือ ความถูกต้องของคำตอบ\n",
        "  - คะแนนที่ได้จาก prompt จะต้องเป็นตัวเลข 0 - 4. 0 คือน้อยสุด (ไม่ถูกต้องเลย) 4 คือมากสุด (ถูกต้องครบถ้วน)\n",
        "  - ตัวเลข จะต้องอยู่ใน <answer> tags\n",
        "- หลังจากตรวจครบแล้วให้หาคะแนนเฉลี่ย และเก็บค่านั้นไว้ ใน column `score`.\n",
        "- สุดท้าย print `DataFrame` ออกมา แบบไหนทำได้ดีที่สุด?\n",
        "- **Note**:\n",
        "  - ใช้เวลาในการรันประมาณ 12 นาที\n",
        "  - ข้อมูลที่เอามาทดสอบ เป็น public data ในเรื่องที่ค่อนข้างจะมีข้อมูลเยอะ (bio) โมเดลน่าจะมีความรู้เพียงพอที่จะได้เกือบหมดด้วยตัวเอง"
      ],
      "metadata": {
        "id": "ynsKa9DMOq7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combinations = [(em, split, k) for em in ['mini','bge'] for split in ['char','recur'] for k in [1, 5]]\n",
        "\n",
        "RAG_results = []\n",
        "\n",
        "for (em, split, k) in tqdm(combinations):\n",
        "      avg = 0\n",
        "      for i in range(len(eval_df)):\n",
        "          ## Get the response from RAG_response\n",
        "          response = ...\n",
        "\n",
        "          eval_prompt = f'''Your prompt here\n",
        "          '''\n",
        "\n",
        "          answer = generate(eval_prompt)\n",
        "\n",
        "          # Extract the number in <answer> tags\n",
        "          match = re.search(r'<answer>(\\d+)</answer>', answer)\n",
        "          if match:\n",
        "              avg += int(match.group(1))\n",
        "\n",
        "      RAG_results.append({'embedding':em,\n",
        "                      'splitting':split,\n",
        "                      'top-k':k,\n",
        "                      'avg_score':avg/len(eval_df)})\n",
        "\n",
        "RAG_results = pd.DataFrame(RAG_results)\n",
        "RAG_results"
      ],
      "metadata": {
        "id": "12wqv7_C7hoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}