{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxZrhmsCyYagETae6PKIy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsa-nut/SF323_CN408_AIEngineer/blob/main/SF323_CN408_HW3_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW3: Prompt Engineering (5 Points)"
      ],
      "metadata": {
        "id": "8_2RDbdbrUbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenRouter Setup"
      ],
      "metadata": {
        "id": "J-prgl0NBvOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=userdata.get('openrouter'),\n",
        ")"
      ],
      "metadata": {
        "id": "L0YoCRFnBw-q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Prompting Techniques (1 points)\n",
        "\n",
        "- Questions are from [Claude Exercises](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing)\n",
        "- We are using `gemini-2.5-flash-lite` for this homework"
      ],
      "metadata": {
        "id": "vLhnNHaHGJCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI25FLASHLITE =\"google/gemini-2.5-flash-lite\"\n",
        "GEMINI25FLASH=\"google/gemini-2.5-flash\"\n",
        "\n",
        "# Helper function. DO NOT CHANGE.\n",
        "# Temperature is set to 0 so that the answer will be the same.\n",
        "def get_response(user_prompt, system_prompt=\"\", prefill=\"\", model=GEMINI25FLASHLITE):\n",
        "  temp_messages =[\n",
        "        {'role':'system','content':system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\":\"assistant\",\"content\":prefill}\n",
        "  ]\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=temp_messages,\n",
        "      temperature = 0,\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "# Testing if your api is working\n",
        "print(get_response(\"Hello\", 'You are a helpful assistance.', prefill='Hi,'))"
      ],
      "metadata": {
        "id": "ZkkHgF6_pYvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfd2736-40e5-4927-82c4-49688998d86f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " how can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.1** Fix the prompt below by adding XML tags so that the model produces the right answer.\n",
        "\n",
        "Try not to change anything else about the prompt. The messy and mistake-ridden writing is intentional, so you can see how the model reacts to such mistakes."
      ],
      "metadata": {
        "id": "_NosBeB7zAe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"ar cn bronw?\"\n",
        "prompt = f\"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgcBpFSpz8-C",
        "outputId": "713a6662-f378-48e8-e83c-bc564b2ad270"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I can help with your dog question! Please ask it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.2** Fix the prompt **WITHOUT adding XML tags**. Instead, remove only one or two words from the prompt."
      ],
      "metadata": {
        "id": "3I2gC1bv0e5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"ar cn bronw?\"\n",
        "prompt = f\"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPvOcOYg0k5Y",
        "outputId": "338e09b2-b3ce-43ff-bf6c-460851fa3b68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I can help with your dog question! Please ask it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.3** Forced to make a choice, **Gemini-2.0-flash designates Michael Jordan as the best basketball** player of all time. Can we get Gemini to pick someone else?\n",
        "\n",
        "Modify the \"best basketball player\" prompt and use the **prefill technique**(putting words after \"Assistant:\") to compell Gemini to make a detailed argument that the best basketball player of all time is **Stephen Curry** ([this guy](https://en.wikipedia.org/wiki/Stephen_Curry)).\n",
        "\n",
        "**Note**: You can be very creative how you prefill."
      ],
      "metadata": {
        "id": "9zeV8Be_112N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who is the best basketball player of all time? Please choose one specific player.\"\n",
        "prefill = \"\"\n",
        "print(get_response(prompt, prefill=prefill))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKPzlu1a2EL9",
        "outputId": "63aae6bb-216e-4312-b274-ce0ef3dff232"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a question that sparks endless debate among basketball fans, and there's no single, universally agreed-upon answer. However, if I have to choose one specific player as the best of all time, I would select **Michael Jordan**.\n",
            "\n",
            "Here's why:\n",
            "\n",
            "*   **Unparalleled Dominance and Winning:** Jordan led the Chicago Bulls to six NBA championships in the 1990s, achieving two separate three-peats. He was the Finals MVP in all six of those championships. This level of sustained success and clutch performance in the biggest moments is unmatched.\n",
            "*   **Individual Accolades:** His trophy cabinet is overflowing: 5 NBA MVP awards, 6 NBA Finals MVP awards, 10 scoring titles (an NBA record), 3 steals titles, and 9 All-Defensive First Team selections. He was also a 14-time All-Star and a 10-time All-NBA First Team selection.\n",
            "*   **Impact on the Game:** Jordan transcended the sport. He popularized basketball globally, made the NBA a worldwide phenomenon, and inspired a generation of players. His athleticism, skill, and competitive fire set a new standard for what was possible on the court.\n",
            "*   **Two-Way Excellence:** While known for his scoring prowess, Jordan was also an elite defender. His ability to dominate on both ends of the floor is a rare and crucial factor in his GOAT status.\n",
            "*   **The \"Killer Instinct\":** Jordan had an almost supernatural ability to perform at his best when the game was on the line. He rarely missed a crucial shot and consistently delivered in clutch situations.\n",
            "\n",
            "While other players like LeBron James, Kareem Abdul-Jabbar, and Bill Russell have incredible arguments and achievements, Jordan's combination of individual brilliance, championship success, and cultural impact makes him my choice for the greatest basketball player of all time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.4** Modify the haiku prompt below so that Gemini produces **two haikus about two different animals**.\n",
        "\n",
        "Use {ANIMAL1} as a stand-in for the first substitution, and {ANIMAL2} as a stand-in for the second substitution."
      ],
      "metadata": {
        "id": "3LSjviFn4fll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANIMAL1 = 'cat'\n",
        "ANIMAL2 = 'dog'\n",
        "prompt = f\"Please write a haiku about {ANIMAL1}. Put it in <haiku> tags.\" #Modify this one\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "Za47Llhv4fag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fbd2ef-b87d-4d99-dfa8-b58a5049aeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```xml\n",
            "<haiku>\n",
            "Soft fur, gentle purr,\n",
            "Eyes of emeralds gleam bright,\n",
            "Sunbeam nap begins.\n",
            "</haiku>\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.5** For this question, we'll be instructing Gemini to sort emails into the following categories:\n",
        "\n",
        "* (A) Pre-sale question\n",
        "* (B) Broken or defective item\n",
        "* (C) Billing question\n",
        "* (D) Other (please explain)\n",
        "\n",
        "For the first part of the exercise, change the prompt below to **make Gemini output the correct classification and ONLY the classification**. Your answer needs to include the letter (A - D) of the correct choice, with the parentheses, as well as the name of the category (e.g. (A) Pre-sale question).\n"
      ],
      "metadata": {
        "id": "gFvvG10q6Dim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint:\n",
        "## 1) How will the model know what categories you want to use?\n",
        "## 2) Be sure to tell to only include the classification\n",
        "## 3) Consider using prefill to force Claude to only response with the classification immedietely. (You don't have to.)\n",
        "prompt = f\"\"\"Please classify this email as either green or blue: {{EMAIL}}\"\"\" #Modify this!\n",
        "prefill = \"\"\n",
        "\n",
        "\n",
        "email1 = \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\"\n",
        "email2 = \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\"\n",
        "email3 = \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\"\n",
        "email4 = \"How did I get here I am not good with computer.  Halp.\"\n",
        "\n",
        "print(get_response(prompt.format(EMAIL=email1), prefill=prefill)) # The answer is B\n",
        "print(get_response(prompt.format(EMAIL=email2), prefill=prefill)) # The answer is D or A\n",
        "print(get_response(prompt.format(EMAIL=email3), prefill=prefill)) # The answer is C\n",
        "print(get_response(prompt.format(EMAIL=email4), prefill=prefill)) # The answer is D"
      ],
      "metadata": {
        "id": "VFvPeqLh4vL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.5**:  Continuing from the last question, we're going to refine the output of the above prompt to yield an answer formatted exactly how we want it. This time, we're going to use \"few-shot\" examples of emails + proper classification (and formatting) to get Gemini to output the correct\n",
        "\n",
        "Provide examples with XML tags to make Gemini wrap just the letter of the correct classification in `<answer></answer>` tags. For instance, the answer to the first email should contain the exact string `<answer>B</answer>`."
      ],
      "metadata": {
        "id": "AXswA4v98V0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: you could ask Gemini to help come with examples if you struggle to come up with one.\n",
        "prompt = f\"\"\"Please classify this email as either green or blue: {{EMAIL}}\"\"\"\n",
        "prefill = \"\" # If you prefill, Gemini won't actually output that as part of its response. (You don't have to.)\n",
        "\n",
        "email1 = \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\"\n",
        "email2 = \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\"\n",
        "email3 = \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\"\n",
        "email4 = \"How did I get here I am not good with computer.  Halp.\"\n",
        "\n",
        "print(get_response(prompt.format(EMAIL=email1), prefill=prefill)) # The answer is B\n",
        "print(get_response(prompt.format(EMAIL=email2), prefill=prefill)) # The answer is D or A\n",
        "print(get_response(prompt.format(EMAIL=email3), prefill=prefill)) # The answer is C\n",
        "print(get_response(prompt.format(EMAIL=email4), prefill=prefill)) # The answer is D"
      ],
      "metadata": {
        "id": "XzyKXGEs8XKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Summarzing articles (1 points)"
      ],
      "metadata": {
        "id": "RM74nAjCCkdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to design an effective prompt that instructs an AI model to analyze a news article and extract information in a specific JSON format. You will be evaluated on how well your prompt helps the AI avoid hallucinations, maintain comprehensive coverage, and follow the exact format requirements.\n",
        "\n",
        "**The Challenge**\n",
        "\n",
        "Given a news article (from CNBC), create a prompt that will generate the following JSON output:\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"author\": \"author of the article\",\n",
        "   \"date\": \"date of the article\",\n",
        "   \"summary\": \"summary\",\n",
        "   \"key_takeaways\": \"key takeaways up to 3 points\",\n",
        "   \"topic\": \"2-4 topics of this article\"\n",
        "}\n",
        "```\n",
        "\n",
        "**Evaluation Criteria**\n",
        "\n",
        "Your prompt's output will be scored (1-10) based on:\n",
        "\n",
        "**Mandatory Requirements (failure = score ≤3):**\n",
        "1. **JSON Format Compliance**: Valid JSON with exactly the required fields\n",
        "2. **No Hallucination**: ALL information must come from the article - any made-up facts result in automatic failure\n",
        "3. **Field Requirements**:\n",
        "   - 'author': Must match article byline (or 'Not specified' if absent)\n",
        "   - 'date': Must match article publication date in the following format: Month Day Year.\n",
        "   - 'summary': Must be present and coherent\n",
        "   - 'key_takeaways': Must be an array with 1-3 points\n",
        "   - 'topic': Must be an array with 2-4 topics\n",
        "\n",
        "**Secondary Criteria:**\n",
        "4. **Coverage**: Summary and takeaways capture all major points\n",
        "5. **Information Accuracy**: Author and date exactly match the source\n",
        "6. **Summary Quality**: 50-150 words, objective, comprehensive\n",
        "7. **Key Takeaways**: Distinct, valuable insights\n",
        "8. **Topic Relevance**: Accurately reflect article themes\n",
        "\n",
        "See the eval prompt below for more detail.\n",
        "\n",
        "There only 5 news in the dataset. Gemini-2.5-flash is used as a judge.\n",
        "\n",
        "**Important note**:\n",
        "- In your prompt, use `{News}` as a placeholder for the actual news from the data.\n",
        "- To get a full score, your solution must get above 5 points on average. (You should be able to get a perfect/near-perfect score.)\n",
        "- You can click \"Convert this dataframe to an interactive dataframe\" to easily see your output"
      ],
      "metadata": {
        "id": "ilinn_Khd-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yongsa-nut/SF323_CN408_AIEngineer/refs/heads/main/HW3_News%20-%20Sheet1.csv"
      ],
      "metadata": {
        "id": "AHZkSxjKaNmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "df = pd.read_csv('HW3_News - Sheet1.csv')\n",
        "news = df.to_dict('records')\n",
        "print(news)"
      ],
      "metadata": {
        "id": "loh8Ol38aSW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Eval Prompt"
      ],
      "metadata": {
        "id": "9rd-14I6Ofru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for the judge LLM\n",
        "\n",
        "summarize_eval_prompt = \"\"\"Your task is to evaluate the following AI-generated article analysis with EXTREME RIGOR.\n",
        "\n",
        "Original task description:\n",
        "<task_description>\n",
        "Extract key information from a CNBC article and provide a summary in the specified JSON format. The output must include author, date, summary, key takeaways (up to 3 points), and topics (2-4).\n",
        "</task_description>\n",
        "\n",
        "Original article:\n",
        "<article>\n",
        "{prompt_inputs}\n",
        "</article>\n",
        "\n",
        "Solution to Evaluate:\n",
        "<solution>\n",
        "{output}\n",
        "</solution>\n",
        "\n",
        "Criteria you should use to evaluate the solution:\n",
        "<criteria>\n",
        "MANDATORY REQUIREMENTS (violation = score 3 or lower):\n",
        "1. JSON Format Compliance: Output must be valid JSON with EXACTLY these fields: 'author', 'date', 'summary', 'key_takeaways', 'topic'\n",
        "2. No Hallucination: ALL information must be directly sourced from the article. Any fabricated facts = automatic failure\n",
        "3. Field Requirements:\n",
        "   - 'author': Must match article byline (or 'Not specified' if absent)\n",
        "   - 'date': Must match article publication date in the following format: Month Day Year.\n",
        "   - 'summary': Must be present and coherent\n",
        "   - 'key_takeaways': Must be an array with 1-3 points\n",
        "   - 'topic': Must be an array with 2-4 topics\n",
        "\n",
        "SECONDARY CRITERIA:\n",
        "4. Coverage Score: Summary and key takeaways must capture the main points:\n",
        "   - Major facts/figures mentioned\n",
        "   - Primary subject matter\n",
        "   - Key outcomes or implications\n",
        "5. Information Accuracy: All extracted data (author, date) must exactly match the source\n",
        "6. Summary Quality:\n",
        "   - Concise (50-150 words)\n",
        "   - Covers the main story without minor details\n",
        "   - Written in third person, objective tone\n",
        "7. Key Takeaways Quality:\n",
        "   - Each point is distinct and valuable\n",
        "   - Captures the most important insights\n",
        "   - Actionable or noteworthy information\n",
        "8. Topic Relevance: Topics accurately reflect the article's subject matter and themes\n",
        "\n",
        "<format_example>\n",
        "{\n",
        "   \"author\": \"Dylan Butts\",\n",
        "   \"date\": \"Jul 17 2024\",\n",
        "   \"summary\": \"Taiwan Semiconductor Manufacturing Company (TSMC) reported a 61% year-over-year increase in second-quarter profit, reaching a record high and exceeding analyst estimates. The surge was driven by strong demand for AI chips. The company forecasts third-quarter revenue between $31.8 billion and $33.0 billion, representing a 38% year-over-year increase. TSMC expects full-year 2025 revenue to grow approximately 30% in U.S. dollar terms.\",\n",
        "   \"key_takeaways\": [\n",
        "      \"TSMC's Q2 profit surged 61% YoY to NT$398.27 billion, beating estimates\",\n",
        "      \"Q3 revenue forecast of $31.8-33.0 billion represents 38% YoY growth\",\n",
        "      \"Full-year 2025 revenue expected to rise 30% driven by AI chip demand\"\n",
        "   ],\n",
        "   \"topic\": [\"semiconductor manufacturing\", \"artificial intelligence\", \"earnings report\", \"technology sector\"]\n",
        "}\n",
        "</format_example>\n",
        "\n",
        "</criteria>\n",
        "\n",
        "Scoring Guidelines:\n",
        "* Score 1-3: Solution violates mandatory requirements (invalid JSON, hallucination, missing required fields)\n",
        "* Score 4-6: Valid format but poor coverage of main points or significant inaccuracies\n",
        "* Score 7-8: Good coverage and accuracy with minor issues in summary/takeaways quality\n",
        "* Score 9-10: Excellent coverage, no hallucination, accurate extraction, high-quality summary\n",
        "\n",
        "IMPORTANT SCORING INSTRUCTIONS:\n",
        "* ANY hallucinated information (facts not in the article) MUST result in a score of 3 or lower\n",
        "* Verify EVERY claim in the summary against the source article\n",
        "* Check that all required JSON fields are present and properly formatted\n",
        "* Coverage assessment: List the main points from the article and check how many appear in the summary/takeaways\n",
        "\n",
        "Output Format\n",
        "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
        "- \"strengths\": An array of 1-3 key strengths\n",
        "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
        "- \"reasoning\": A concise explanation of your overall assessment including specific coverage and hallucination checks\n",
        "- \"score\": A number between 1-10\n",
        "\n",
        "Respond with JSON. Keep your response concise and direct.\n",
        "Example response shape:\n",
        "{\n",
        "      \"strengths\": string[],\n",
        "      \"weaknesses\": string[],\n",
        "      \"reasoning\": string,\n",
        "      \"score\": number\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kF1qN6t3ehNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "1o_k_xhhOaaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "# Function to fill values in { } in the prompt template\n",
        "def fill_prompt(template_string, variables):\n",
        "    placeholders = re.findall(r\"{([^{}]+)}\", template_string)\n",
        "\n",
        "    result = template_string\n",
        "    for placeholder in placeholders:\n",
        "         if placeholder in variables:\n",
        "            result = result.replace(\n",
        "                \"{\" + placeholder + \"}\", str(variables[placeholder])\n",
        "            )\n",
        "\n",
        "    return result.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
        "\n",
        "# Basic function to do evals\n",
        "def gen_eval(prompt, testset, eval_prompt):\n",
        "    results = []\n",
        "    for test in tqdm(testset):\n",
        "        filled_prompt = fill_prompt(prompt, test)\n",
        "        response = get_response(filled_prompt)\n",
        "\n",
        "        temp_variables = {\n",
        "            'prompt_inputs':filled_prompt,\n",
        "            'output':response\n",
        "        }\n",
        "        filled_eval_prompt = fill_prompt(eval_prompt, temp_variables)\n",
        "        eval_result = get_response(filled_eval_prompt, model=GEMINI25FLASHLITE)\n",
        "\n",
        "        eval_result = json.loads(eval_result.replace('```json', '').replace('```', ''))\n",
        "        result = temp_variables | eval_result\n",
        "        results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "tA-ZHwY2lQZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Your prompt"
      ],
      "metadata": {
        "id": "-KM9MjGHOcsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_summarized_prompt = \"\"\"Your prompt here\n",
        "\n",
        "{News}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "summarized_results = gen_eval(your_summarized_prompt, news, summarize_eval_prompt)\n",
        "print(f\"\\n\\nmean score = {summarized_results['score'].mean()}\\n\")\n",
        "summarized_results"
      ],
      "metadata": {
        "id": "e4BjUvM1CniY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: PPI (1 points)\n",
        "\n",
        "### Objective\n",
        "Design and implement a prompt that instructs Gemini to anonymize personally identifiable information (PII) in customer feedback while preserving product names and message content integrity. You must create a prompt that can consistently and accurately anonymize various types of PII according to specific rules.\n",
        "\n",
        "### Task Requirements\n",
        "\n",
        "Create a prompt that instructs an LLM to anonymize customer feedback according to the following specifications:\n",
        "\n",
        "#### Anonymization Rules:\n",
        "1. **Customer Names**: Replace all customer names with `CUSTOMER_[ID]`\n",
        "   - This includes both message authors (names before \":\") and any mentions within messages\n",
        "   - Maintain consistent IDs for the same person across messages\n",
        "\n",
        "2. **Employee Names**: Replace with `EMPLOYEE_[ID]`\n",
        "   - Look for names preceded by job titles or role descriptions (rep, technician, agent, support, etc.)\n",
        "\n",
        "3. **Email Addresses**: Replace with `EMAIL_[ID]@anonymized.com`\n",
        "\n",
        "4. **Phone Numbers**: Replace with `PHONE_[ID]`\n",
        "   - Handle various formats: xxx-xxx-xxxx, (xxx) xxx-xxxx, etc.\n",
        "\n",
        "5. **Company Names**: Replace with `COMPANY_[ID]`\n",
        "   - Look for company indicators: Industries, Corp, Corporation, Inc, LLC, etc.\n",
        "\n",
        "6. **DO NOT Anonymize**:\n",
        "   - Product names (AcmeCloud, AcmePro, AcmeSync, Madison, Victoria, Jordan, Morgan, Spencer, Taylor, Cameron)\n",
        "   - Anonymous entries (messages starting with \"Anonymous:\")\n",
        "   - Generic references without names (\"your support team\")\n",
        "\n",
        "#### Output Format:\n",
        "- Each anonymized message should be preserved in its entirety\n",
        "- Messages should be separated by \"---\"\n",
        "- Maintain the original message structure and content\n",
        "\n",
        "### Evaluation Criteria\n",
        "The output will be evaluated using LLM-as-judge (see `eval_prompt` below) with the following point distribution:\n",
        "\n",
        "- **Customer Names (2 pts)**: All customer names properly anonymized\n",
        "- **Employee Names (2 pts)**: All employee/staff names properly anonymized  \n",
        "- **Email Addresses (2 pts)**: All emails properly anonymized\n",
        "- **Phone Numbers (2 pts)**: All phone numbers properly anonymized\n",
        "- **Product Preservation (1 pt)**: No product names incorrectly anonymized\n",
        "- **Output format (1 pt)**: Each message is separated by \"---\"\n",
        "\n",
        "**Note**: Each criterion uses all-or-nothing scoring. Missing even one instance results in 0 points for that category.\n",
        "\n",
        "### Example Expected Output\n",
        "For the message:\n",
        "```\n",
        "Megan Hall: I spoke with your technician, Brian Walker, yesterday. He fixed everything remotely. Amazing service!\n",
        "```\n",
        "\n",
        "Expected anonymization:\n",
        "```\n",
        "CUSTOMER_001: I spoke with your technician, EMPLOYEE_001, yesterday. He fixed everything remotely. Amazing service!\n",
        "```\n",
        "\n",
        "### Important note:\n",
        "- In your prompt, use `{feedback}` as a placeholder for the actual feedback from the data.\n",
        "- **To get a full point**, your prompt must score above 6 on average. (You should be able to get near-perfect/perfect score. This task is easy for gemini-2.0-flash)\n",
        "- Gemini-2.5-flash-lite is used as a judge same as the previous question.\n",
        "\n",
        "### Hints\n",
        "- Check out the examples below.\n",
        "- Read the `eval_prompt`"
      ],
      "metadata": {
        "id": "7Q_rJXVVChvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPI test data"
      ],
      "metadata": {
        "id": "xAWf7R-ZOVFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PPI_test = data_examples = [\n",
        "    # Example 1\n",
        "    {\n",
        "        'feedback' : \"\"\"Jane Smith: I've been using AcmeCloud for years, and it's fantastic! But the latest update has some bugs.\n",
        "\n",
        "Bob Johnson: Your support team is terrible. I called 555-123-4567 three times and no one answered. My email is bob.j@email.com if you actually want to help.\n",
        "\n",
        "Anonymous: The pricing is way too high for what you offer.\"\"\"\n",
        "    },\n",
        "\n",
        "    # Example 2\n",
        "    {\n",
        "        'feedback' : \"\"\"Robert Taylor: Victoria (the AI assistant) is amazing! Much better than the competition. You can reach me at (555) 987-6543 for a testimonial.\n",
        "\n",
        "Emily Davis: Why did you remove the batch processing feature from AcmePro? This is essential for my work!\n",
        "\n",
        "Dr. Jennifer Lee: I'm having issues with data sync. Contact me at jlee@hospital.org or 555-234-5678 to resolve this ASAP.\"\"\"\n",
        "    },\n",
        "\n",
        "    # Example 3\n",
        "    {\n",
        "        'feedback' : \"\"\"David Kim: I can't believe Morgan (your project management tool) doesn't have dark mode in 2024! My eyes are killing me.\n",
        "\n",
        "Anonymous: Best software I've ever used. Worth every penny!\n",
        "\n",
        "James Cooper: Your support agent Ashley Chen went above and beyond! Email me at j.cooper@email.net for more feedback.\"\"\"\n",
        "    },\n",
        "\n",
        "    # Example 4\n",
        "    {\n",
        "        'feedback' : \"\"\"Christopher Adams: The export feature in AcmeSync is broken. PDFs come out all garbled. Fix this please!\n",
        "\n",
        "Megan Hall: I spoke with your technician, Brian Walker, yesterday. He fixed everything remotely. Amazing service!\n",
        "\n",
        "Ryan Martinez: I've been on hold for 2 hours trying to reach someone at 555-345-6789. This is ridiculous!\"\"\"\n",
        "    },\n",
        "\n",
        "    # Example 5\n",
        "    {\n",
        "        'feedback' : \"\"\"Anonymous: When will you add API access to AcmePro?\n",
        "\n",
        "Sophia Lewis: This is Sophia (sophia.lewis@startup.io). I found a security vulnerability in Cameron (the reporting module). Please contact urgently!\n",
        "\n",
        "Daniel Robinson: Why does AcmeCloud require so many permissions? Seems excessive for what it does.\"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "VwJqt4b99lWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval Prompt"
      ],
      "metadata": {
        "id": "waYa8d3vOGmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval prompt\n",
        "PPI_eval_prompt = \"\"\"Your task is to evaluate the following PII anonymization output with EXTREME RIGOR.\n",
        "\n",
        "Original task description:\n",
        "<task_description>\n",
        "Anonymize customer feedback by replacing all personally identifiable information (PII) while preserving product names and message content. Each anonymized message should be separated by \"---\".\n",
        "</task_description>\n",
        "\n",
        "Original data:\n",
        "<original_data>\n",
        "{prompt_inputs}\n",
        "</original_data>\n",
        "\n",
        "Solution to Evaluate:\n",
        "<solution>\n",
        "{output}\n",
        "</solution>\n",
        "\n",
        "Anonymization Criteria and Scoring:\n",
        "<criteria>\n",
        "1. CUSTOMER NAMES (2 points - CRITICAL):\n",
        "   - All customer names (both message authors before \":\" and names mentioned within messages) must be replaced with CUSTOMER_[ID]\n",
        "   - Same customer mentioned multiple times must have consistent ID\n",
        "   - Missing even ONE instance = 0 points\n",
        "   - Example: \"Jane Smith: This is Jane...\" → \"CUSTOMER_001: This is CUSTOMER_001...\"\n",
        "\n",
        "2. EMPLOYEE/STAFF NAMES (2 points - CRITICAL):\n",
        "   - Names with job titles/roles (rep, technician, agent, support, etc.) must be replaced with EMPLOYEE_[ID]\n",
        "   - Missing even ONE instance = 0 points\n",
        "   - Example: \"your technician, Brian Walker\" → \"your technician, EMPLOYEE_001\"\n",
        "\n",
        "3. EMAIL ADDRESSES (2 points - CRITICAL):\n",
        "   - All email addresses must be replaced with EMAIL_[ID]@anonymized.com\n",
        "   - Missing even ONE instance = 0 points\n",
        "   - Example: \"sarah@techcorp.net\" → \"EMAIL_001@anonymized.com\"\n",
        "\n",
        "4. PHONE NUMBERS (2 points - CRITICAL):\n",
        "   - All phone numbers (any format) must be replaced with PHONE_[ID]\n",
        "   - Missing even ONE instance = 0 points\n",
        "   - Example: \"(555) 987-6543\" → \"PHONE_001\"\n",
        "\n",
        "5. PRODUCT PRESERVATION (1 point):\n",
        "   - Product/feature names (AcmeCloud, Madison, Jordan, etc.) must NOT be anonymized\n",
        "   - These should remain intact even if they sound like person names\n",
        "   - Anonymizing any product name = 0 points\n",
        "   - Example: \"Jordan (the analytics module)\" should remain unchanged\n",
        "\n",
        "6. OUTPUT FORMAT (1 point):\n",
        "   - Separate each message with \"---\"\n",
        "\n",
        "SPECIAL CASES:\n",
        "- Anonymous entries: Messages starting with \"Anonymous:\" should remain as-is\n",
        "- Company names with indicators (Industries, Corp, etc.) should be replaced with COMPANY_[ID]\n",
        "- Third-party names (non-employees) should be replaced with PERSON_[ID]\n",
        "\n",
        "SCORING RULES:\n",
        "- Each criterion is all-or-nothing (full points or 0)\n",
        "- Total possible score: 10 points\n",
        "- ANY missed PII instance in a category results in 0 points for that category\n",
        "</criteria>\n",
        "\n",
        "Evaluation format:\n",
        "Provide your evaluation as a structured JSON object with the following fields:\n",
        "- \"criteria_scores\": An object mapping each criterion to its score (e.g., {\"customer_names\": 3, \"employee_names\": 0, ...})\n",
        "- \"missed_instances\": An array listing specific PII instances that were not anonymized\n",
        "- \"incorrectly_anonymized\": An array listing items that were anonymized but shouldn't have been (e.g., product names)\n",
        "- \"total_score\": The sum of all criteria scores (out of 10)\n",
        "\n",
        "Respond with JSON only.\n",
        "\n",
        "Example response shape:\n",
        "{\n",
        "    \"criteria_scores\": {\n",
        "        \"customer_names\": 0,\n",
        "        \"employee_names\": 2,\n",
        "        \"email_addresses\": 2,\n",
        "        \"phone_numbers\": 0,\n",
        "        \"product_preservation\": 1,\n",
        "        \"output_format\": 0\n",
        "    },\n",
        "    \"missed_instances\": [\n",
        "        \"Customer name 'Sarah' within message 1 was not replaced\",\n",
        "        \"Phone number '555-123-4567' in message 2 was not anonymized\",\n",
        "        \"missing '---' between messages\"\n",
        "    ],\n",
        "    \"incorrectly_anonymized\": [],\n",
        "    \"total_score\": 5\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7BIiIb9q9o73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your prompt"
      ],
      "metadata": {
        "id": "CrpnmcH9OKND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PPI_your_prompt = \"\"\"Your prompt here.\n",
        "\n",
        "{feedback}\n",
        "\"\"\"\n",
        "\n",
        "# Don't change anything below\n",
        "PPI_results = gen_eval(PPI_your_prompt, PPI_test, PPI_eval_prompt)\n",
        "print(f\"\\n\\nmean score = {PPI_results['total_score'].mean()}\\n\")\n",
        "PPI_results"
      ],
      "metadata": {
        "id": "LkkV8CCy9i5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Classify Customer Intent (2 points)\n",
        "\n",
        "For this problem, your task is to create a prompt for Gemini 2.0 flash to classify the customer query into category and intent. The dataset is from https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset.\n",
        "\n",
        "For this problem, we will only use the subset of this data which only has 3 categories and 9 intents as follows:\n",
        "\n",
        "* ORDER category\n",
        "  * cancel_order\n",
        "  * change_order\n",
        "  * place_order\n",
        "  * track_order\n",
        "* PAYMENT category\n",
        "  * check_payment_methods\n",
        "  * payment_issue\n",
        "* REFUND category\n",
        "  * check_refund_policy\n",
        "  * get_refund\n",
        "  * track_refund\n",
        "\n",
        "For each intent, there are 5 data points (total 45).\n",
        "\n",
        "**Input-Ouput Requirement:**\n",
        "\n",
        "- Input will be pass to `{{QUERY}}` so your prompt must include it.\n",
        "- Output for category must be inside `<category>` tags and for output for intent must be inside `<intent>` tags\n",
        "- See the code below for more details\n",
        "- It can take up to 5 mins to finish generating 45 responses. You may not want to wait until the end if your prompt doesn't work correctly. You can stop and change your prompt.\n",
        "\n",
        "**Evaluation:**\n",
        "- The code for evaluation is provided for you.\n",
        "- For this problem, we will check two things: the accuracy in predicting category and the accuracy in predicting intent. You should be able to get close to 100%.\n",
        "- Point Breakdown:\n",
        "  - Category/Intent accuracy >= 0.9: 1 each\n",
        "  - Category/Intent accuracy = [0.8, 0.9): 0.5 each\n",
        "  - Category/Intent accuracy = [0.5, 0.8): 0.25 each\n",
        "  - Category/Intent accuracy = [0.0, 0.5): 0\n"
      ],
      "metadata": {
        "id": "buq4w8ncFnyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yongsa-nut/SF323_CN408_AIEngineer/refs/heads/main/subset_customer_support.csv"
      ],
      "metadata": {
        "id": "oO4cR_NWlUzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "8mcGMensht5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "# You will get a warning but you can simply ignore it.\n",
        "df = pd.read_csv(\"subset_customer_support.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "EcoxkvKoFngf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Your prompt here**\n",
        "- Feel free to do whatever you want"
      ],
      "metadata": {
        "id": "d3DxrXazhbbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in your prompts here\n",
        "prompt = \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\n",
        "prefill = \"\""
      ],
      "metadata": {
        "id": "lPa-mNw6hg8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prediction and Evaluation**\n",
        "- Once you fill out your prompt simply run the code below"
      ],
      "metadata": {
        "id": "yVbz7gNohofR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to test your prompt quickly use this block\n",
        "index = 0 # change this to test different data (0 - 44)\n",
        "fill_prompt = prompt.replace('{{QUERY}}', df['instruction'][index])\n",
        "response = get_response(fill_prompt, system_prompt, prefill)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ig1SEzz5jDE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pause = 5 # Pause time to not exceed rate limit. Increase this number if you has issue with rate limit\n",
        "\n",
        "result_df = df.copy()\n",
        "result_df['predicted_category'] = ''\n",
        "result_df['predicted_intent'] = ''\n",
        "\n",
        "# Then we generate predictions\n",
        "\n",
        "def predict(prompt, system_prompt, prefill, df, echo=True):\n",
        "  for i in range(len(df)):\n",
        "    fill_prompt = prompt.replace('{{QUERY}}', df['instruction'][i])\n",
        "    response = get_response(fill_prompt, system_prompt, prefill)\n",
        "\n",
        "    # Extract XML tags\n",
        "    intent = re.search('<intent>(.*?)</intent>', response).group(1)\n",
        "    df.loc[i, 'predicted_intent'] = intent\n",
        "    category = re.search('<category>(.*?)</category>', response).group(1)\n",
        "    df.loc[i, 'predicted_category'] = category\n",
        "\n",
        "    if echo:\n",
        "      print(\"Query:\", df['instruction'][i])\n",
        "      print(response)\n",
        "\n",
        "    time.sleep(pause) # waiting\n",
        "\n",
        "# Then we generate predictions\n",
        "predict(prompt, system_prompt, prefill, result_df, echo = True)"
      ],
      "metadata": {
        "id": "prKKqz8dh-_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the prediction\n",
        "result_df"
      ],
      "metadata": {
        "id": "cFZOZ58BiAbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "def accuracy_to_point(accuracy):\n",
        "  if accuracy >= 0.9:\n",
        "    return 1\n",
        "  elif accuracy > 0.8:\n",
        "    return 0.5\n",
        "  elif accuracy > 0.5:\n",
        "    return 0.25\n",
        "  return 0\n",
        "\n",
        "def evaluation(df):\n",
        "  acc_category =sum(df['category'] == df['predicted_category']) / len(df)\n",
        "  acc_intent = sum(df['intent'] == df['predicted_intent']) / len(df)\n",
        "  return acc_category, acc_intent\n",
        "\n",
        "acc_category, acc_intent = evaluation(result_df)\n",
        "print(f\"Accuracy in predicting category: {acc_category:.2f}\")\n",
        "print(f\"Accuracy in predicting intent: {acc_intent:.2f}\")\n",
        "\n",
        "print(f'Total point: {accuracy_to_point(acc_category) + accuracy_to_point(acc_intent)}')"
      ],
      "metadata": {
        "id": "IP7Nn_JwiBc6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}